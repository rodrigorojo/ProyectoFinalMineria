\documentclass{article}
\usepackage[left=2cm,right=2cm,top=3cm,bottom=3cm,letterpaper]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}

\title{Almacenes y Minería de Datos \\Proyecto Final\\ Entregable 1}
\author{Adolfo Marín Arriaga \and Juan Carlos López López \and Luis Rodrigo Rojo Morales}
\date{\today\\}

\begin{document}
 \maketitle

 {\bf Algoritmo 1: Naive Bayes}\\

 {\bf Objetivo}\\

 El objetivo de Naive Bayes o Clasificador bayesiano ingenuo es la construccion de clasificadores en base a datos que ya se conocen por ejemplo clasificar si una persona es hombre o mujer basandose
 en los datos que se tienen de su altura, peso y tamaño del pie. Se usa basandose en el teorema de Bayes y asume que las variables que se usan para predecir son
 independientes\\

 {\bf Descripción}\\

 Lo que tenemos es un objeto que es lo que se quiere clasificar, las variables que se van a usar para predecir que son los datos que ya conocemos y distintas clases a donde podria
 ir cada objeto después de la clasificación. Se usa la regla de bayes que dice que si se tienen 2 eventos A y B se saca la probabilidad de A dado B como:
 $$ P(A \mid B) = \frac{P(B \mid A) \, P(A)}{P(B)} $$
 \begin{itemize}
  \item $P(A \mid B)$ Es la probabilidad de que se cumpla  A dado B, es la probabilidad a posteriori.
  \item $P(B \mid A)$ Es la probabilidad de B dado que se cumple A.
  \item $P(A)$ y $P(B)$ Son las probabilidades que se cumplan A y B respectivamente, estas son la probabilidad a priori
 \end{itemize}
 Para el clasificador bayesiano ingenuo hay distintas clases en las que puede caer un objeto después de ser clasificado y distintas variables predictivas, entonces
 si tenemos un conjunto de variables predictivas $X=\{x_1,...,x_n\}$ y m clases para la clasificación $C_1,...,C_m$, el clasificador dice que la
 probabilidad de que un objeto pertenezca a la clase $C_k$ se calcula como:
 \begin{equation*}
 p(C_k|X)=\frac{1}{Z}p(C_k)\prod_{i=1}^{n}p(x_i|C_k)
 \end{equation*}
 \begin{itemize}
  \item $X$ es el conjunto de variables predictivas $X=\{x_1,...,x_n\}$.
  \item $C_k$ es una clase de $C_1,...,C_m$
  \item $Z$ es igual a un factor que depende solo de $x_1,...,x_n$, es una constante si sus valores son conocidos

 \end{itemize}
{\bf Ejemplo:}\\

 Un ejemplo es si queremos clasificar frutas, las clases son platano, naranja y otras, se hizó un conjunto de entrenamiento con las variables
 de longitud, sabor y color y nos dio que:
 \begin{center}
  \begin{tabular}{|l||l|l||l|l||l|l||l|}
   \hline
   Fruta & Alargada & No Alargada & Dulce & No Dulce & Amarilla & No Amarilla & Total \\ \hline \hline
   Platano & 400 & 100 & 350 & 150 & 450 & 50 & 500 \\ \hline
   Naranja & 0 & 300 & 150 & 150 & 300 & 0 & 300 \\ \hline
   Otras & 100 & 100 & 150 & 50 & 50 & 150 & 200 \\ \hline \hline
   Total & 500 & 500 & 650 & 350 & 800 & 200 & 1000 \\ \hline
  \end{tabular}
  \end{center}
  Ahora queremos predecir en que clase cae una nueva fruta que llega y es amarilla, alargada y dulce.
  Calculamos las probabilidades:
  \begin{itemize}
   \item $ P(platano) = 500/1000 = $0.5
   \item $ P(platano \mid alargada) = \frac{P(alargada \mid platano) \, P(platano)}{P(alargada)} = \frac{400/500 \, \cdot 500/1000}{500/1000} = $0.8
   \item $ P(platano \mid dulce) = \frac{P(dulce \mid platano) \, P(platano)}{P(dulce)} = \frac{350/500 \, \cdot 500/1000}{650/1000} = $0.5384
   \item $ P(platano \mid amarilla) = \frac{P(amarilla \mid platano) \, P(platano)}{P(amarilla)} = \frac{450/500 \, \cdot 500/1000}{800/1000} = $0.5625
   \item $ P(naranja) = 300/1000 = $0.3
   \item $ P(naranja \mid alargada) = \frac{P(alargada \mid naranja) \, P(naranja)}{P(alargada)} = \frac{0/300 \, \cdot 300/1000}{500/1000} = $0
   \item $ P(naranja \mid dulce) = \frac{P(dulce \mid naranja) \, P(naranja)}{P(dulce)} = \frac{150/300 \, \cdot 300/1000}{650/1000} = $0.2307
   \item $ P(naranja \mid amarilla) = \frac{P(amarilla \mid naranja) \, P(naranja)}{P(amarilla)} = \frac{300/300 \, \cdot 300/1000}{800/1000} = $0.375
   \item $ P(otras) = 200/1000 = $0.2
   \item $ P(otras \mid alargada) = \frac{P(alargada \mid otras) \, P(otras)}{P(alargada)} = \frac{100/200 \, \cdot 200/1000}{500/1000} = $0.2
   \item $ P(otras \mid dulce) = \frac{P(dulce \mid otras) \, P(otras)}{P(dulce)} = \frac{150/200 \, \cdot 200/1000}{650/1000} = $0.2307
   \item $ P(otras \mid amarilla) = \frac{P(amarilla \mid otras) \, P(otras)}{P(amarilla)} = \frac{50/200 \, \cdot 200/1000}{800/1000} = $0.0625
  \end{itemize}
  Ahora para el clasificador con su fórmula:
  \begin{itemize}
   \item $p(platano|alargada,dulce,amarilla)=\frac{1}{Z}p(platano)p(platano|alargada)p(platano|dulce)p(platano|amarilla)$ \\=
   $\frac{1}{Z}$0.5 $\cdot$ 0.8 $\cdot$ 0.5384 $\cdot$ 0.5625 = $\frac{1}{Z}$0.121147
   \item $p(naranja|alargada,dulce,amarilla)=\frac{1}{Z}p(naranja)p(naranja|alargada)p(naranja|dulce)p(naranja|amarilla)$ \\=
   $\frac{1}{Z}$0.5 $\cdot$ 0 $\cdot$ 0.2307 $\cdot$ 0.375 = $\frac{1}{Z}$0
   \item $p(otras|alargada,dulce,amarilla)=\frac{1}{Z}p(otras)p(otras|alargada)p(otras|dulce)p(otras|amarilla)$ \\=
   $\frac{1}{Z}$0.2 $\cdot$ 0.2 $\cdot$ 0.2307 $\cdot$ 0.0625 = $\frac{1}{Z}$0.00057675
  \end{itemize}
  Conocemos la probabilidad de que lo que recibamos sea dulce, alargado y amarillo entonces tomamos $Z$ como:
  \begin{itemize}
   \item $p(alargada)p(dulce)p(amarilla) = \frac{500}{1000}\cdot\frac{650}{1000}\cdot\frac{800}{1000}$=0.26
  \end{itemize}
  Entonces reemplazando $Z$
  \begin{itemize}
   \item $p(platano|alargada,dulce,amarilla) = \frac{0\ldotp121147}{0\ldotp26}$=0.46595
   \item $p(naranja|alargada,dulce,amarilla) = \frac{0}{0\ldotp26}$=0
   \item $p(otras|alargada,dulce,amarilla) = \frac{0\ldotp00057675}{0\ldotp26}$=0.2218
  \end{itemize}
  La probabilidad mas grande es la de que sea platano, entonces si llega una fruta que sea alargada, dulce y amarilla la clasificamos como que va a ser un
  platano.\\

{\bf Bibliografía}
\begin{itemize}
 \item https://en.wikipedia.org/wiki/Naive\_Bayes\_classifier
 \item https://en.wikipedia.org/wiki/Bayes\%27\_theorem
\end{itemize}

{\bf Plan de trabajo}\\

Para nuestro problema en el preprocesamiento de datos es necesario hacer limpieza en los campos workclass, occupation y native-country, porque en algunas
filas tienen un ? en vez de alguno de los datos de las opciones que se dan. \\
\begin{center}
 \includegraphics[scale=0.3]{trabajos}\\
\end{center}

También para los datos que son numéricos los datos varian en muchos valores por ejemplo
edades tiene 71 valores distintos, entonces para estos valores numéricos para que no halla tantas probabilidades, lo mejor para hacer naive bayes seria hacer
discretización de los valores numéricos como por ejemplo con binning para tomarlos por rangos. Así en vez de tomar los 71 valores posibles de edades, tomaría una
menor cantidad de rangos.\\
\begin{center}
 \includegraphics[scale=0.3]{edades}\\
\end{center}
Los datos que no son númericos si caen en opciones predefinidas por lo que salvo los campos que tienen valores con ? no hay inconsistencias en estos datos.
También otros problemas que hay en los datos que son numéricos es que hay muchos outliers, por ejemplo en horas a la semana la mayoria trabaja alrededor de 40
pero hay datos que van desde 1 hasta 99.\\
\begin{center}
 \includegraphics[scale=0.3]{horas}\\
\end{center}
En este caso los outliers se podrían en rangos que sean mayor que un valor y menor que un valor en la discretización que se había planteado previamente.
\\

{\bf Pseudocódigo}
\begin{verbatim}
  1.- Convertir el conjunto de datos en una tabla de frecuencias
  2.- Crear una tabla de probabilidades TP
  3.- Aplicar la formula de Bayes para calcular la probabilidad de cada clase
   con ayuda de TP
  4.- Aplicar la formula del clasificador de naive bayes
  5.- Regresa la clase con la probabilidad mas alta
\end{verbatim}

{\bf Algoritmo 2: C4.5}\\
{\bf Objetivo}\\
Generar un árbol de decisión, por tanto es útil en problemas de clasificación, y por esta razón, C4.5 está casi siempre referido como un clasificador estadístico.\\
 {\bf Descripción}\\
 Para mencionar la forma de trabajar de este algoritmo en primer lugar hay que mencionar que lasteorías de Shannon son la base del algoritmo C4.5. La entropía de Shannon es la más conocido y más aplicado. Ésta define la cantidad de información proporcionada por un evento.\\

Entropía de Shannon:\\
En general, si damos una distribución de probabilidad \[P = (p_1, p_2,..., p_n)\] y un ejemplar S entonces la información acarreada por esta distribución, también llamada entropía de P está dada por:\\
 \begin{equation*}
 Entropia(P) = -\sum_{i=1}^{n}p_ilog(p_i)
 \end{equation*}
Ganancia de Información G(p,T):\\
Tenemos funciones que nos permiten medir el grado de clases mezcladas para todas las muestras y por tanto cualquier posición de el árbol en la construcción. Queda por definir una función para seleccionar la prueba que debe etiquetar al nodo actual.\\
Lo siguiente define una ganancia para una prueba T y su posición p
 \begin{equation*}
 Ganancia(p,T) = -Entropia(p) - \sum_{j=1}^{n}(p_jEntropia(p_j))
 \end{equation*}
donde los valores (pj) es el conjunto de todos los posibles valores para el atributo T.
Podemos usar esta medida para clasificar atributos y construir el árbol de decisión donde en cada nodo se encuentra su atributo con la mayor ganancia de información entre los atributos.\\
C4.5 utiliza la “ganancia de información”, que permite medir una razón de ganancia.
 La razón de ganancia se define como:\\
 $$ RazonGanancia(p,T) = \frac{Ganancia(p,T)}{DivideInfo(p,T)} $$
 donde DivideInfo es:\\
 \begin{equation*}
 DivideInfo(p,test) = - \sum_{j=1}^{n}p'\frac{j}{p}log(p'(\frac{j}{p}))
 \end{equation*}
donde: $$ p'(\frac{j}{p})$$
es la proporción de los elementos presentes en la posición de p, tomando el valor de la prueba j-ésima. La definición anterior es independiente de la distribución de ejemplos dentro de las diferentes clases.\\
Entonces los datos se clasifican en cada nodo del árbol con el fin de determinar el mejor atributo de división. Utiliza el método de la razón de ganancia para evaluar el atributo de división.
Los árboles de decisión se construyen en C4.5 mediante el uso de un conjunto de datos de entrenamiento o conjuntos de datos. En cada nodo de la árbol, C4.5 elige un atributo de los datos que más eficazmente divida su conjunto de muestras en subconjuntos.  Su criterio es la ganancia de información normalizada (Diferencia de entropía) que resulta de la elección de una atributo para dividir los datos. El atributo con la ganancia de información normalizada más alta es elegido para tomar la decisión.\\

 {\bf Ejemplo}\\
 Conjunto de datos S:
\begin{center}
 \includegraphics[scale=0.65]{conjunto}\\
\end{center}

Entropía(S) = -9/14*log2(9/14)-5/14*log2(5/14) = 0.94

Cálculos para los primeros atributos:\\
Ganancia(S, Outlook) = Entropía(S) – 5/14*Entropía $(S_Sun)$
					- 4/14*Entropía $(S_Rain)$
					- 5/14*Entropía $(S_Overcast)$
					= 0.94 – 5/14*0.9710 – 4/14*0 – 5/14*0.9710

			Gain(S, Outlook) = 0.246\\

Cálculos de Entropías:\\
Entropía$(S_Sun)$ = - 2/5*log2(2/5) – 3/5*log2(3/5) = 0.9710\\
Entropía$(S_Rain)$ = -4/4*log2(4/4) – 0*log2(0) = 0\\
Entropía$(S_Overcast)$ = - 3/5*log2(3/5) – 2/5*log2(2/5) = 0.9710\\

Ahora las ganancias de forma similar que se hizo para Ganancia(S, Outlook)\\
Ganancia(S, Wind) = 0.048\\
Ganancia(S, Temperature) = 0.0289\\

Como Humidity tiene valores continuos, la ganancia se calcula de la siguiente forma:\\


Ganancia(S, Humidity) = ?\\\\
Apoyándonos en el conjunto de datos S  hay que ordenar los valores del atributo Humidity en orden ascendente, el conjunto de valores es el siguiente:\\
$[65, 70, 70, 70, 75, 78, 80, 80, 80, 85, 90, 90, 95, 96]$\\\\
Y removemos los valores repetidos:\\
$[65, 70, 75, 78, 80, 85, 90, 95, 96]$\\\\
Utilizando el algoritmo C4.5 la ganancia para el atributo continuo Humidity:\\
\begin{center}
 \includegraphics[scale=0.7]{c45}\\
\end{center}
Ganancia(S, Humidity) = 0.102\\

Entonces quien tiene la mayor ganancia de información es el nodo raíz del árbol de decisión C4.5

\begin{center}
 \includegraphics[scale=0.7]{arbol}\\
\end{center}

{\bf Bibliografía}
\begin{itemize}
 \item https://en.wikipedia.org/wiki/C4.5\_algorithm
 \item Articulo: A comparative study of decision tree ID3 and C4.5. Autores: Badr HSSINA, Abdelkarim MERBOUHA,Hanane EZZIKOURI,Mohammed ERRITALI
\end{itemize}


{\bf Plan de trabajo}\\

Apoyandonos de las gŕaficas obtenidas anteriormente en el algoritmo 1 y dado que el algoritmo C4.5 es una mejora de ID3, tenemos que en éste podemos:
\begin{itemize}
\item Usar datos continuos
\item Manejar datos de entrenamiento con valores faltantes
\item Usar atributos con distintos valores
\end{itemize}

Entonces dado que en nuestro conjunto de datos fuente tenemos valores desconocidos (?) podríamos no hacer limpieza de datos en los campos workclass, occupation y native-country y así ejecutar el algoritmo.\\

También habría necesidad de hacer discretización de los datos ya que algunos rangos de atributos difieren demasiado.

{\bf Pseudocódigo}\\
Los casos base de este algoritmo son:
\begin{itemize}
  \item Todas las muestras en la lista pertenecen a la misma clase. Cuando esto sucede, simplemente crea un nodo de hoja para el árbol de decisión diciendo que elija esa clase.
  \item Ninguna de las características proporciona ninguna ganancia de información. En este caso, C4.5 crea un nodo de decisión más arriba del árbol utilizando el valor esperado de la clase.
  \item Instancia de la clase previamente no vista encontrada. Una vez más, C4.5 crea un nodo de decisión más arriba en el árbol con el valor esperado.
\end{itemize}
Entonces el pseudocódigo nos queda de la siguiente manera:
\begin{verbatim}
  1.- Comprobar los casos base
  2.- for( a : atributos)
  3.-    Encontrar la ganancia de información normalizada de la división de a
  4.- a_best = el atributo con la ganancia de información normalizada más alta
  5.- nuevonodo = nodo de decisión que divida a_best
  6.- Repetir en las sublistas obtenidas por división de a_best,
       y agregar estos nodos como hijos de nodo
\end{verbatim}
\end{document}
